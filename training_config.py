MODEL_NAME = "gpt2" 
MAX_LENGTH = 512

TEST_SIZE = 0.1  
RANDOM_SEED = 42

LEARNING_RATE = 2e-5
WEIGHT_DECAY = 0.01
NUM_EPOCHS = 3
WARMUP_STEPS = 1000
WARMUP_RATIO = 0.1

PER_DEVICE_TRAIN_BATCH_SIZE = 2
PER_DEVICE_EVAL_BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 16

EVAL_STEPS = 250
LOGGING_STEPS = 50
SAVE_STEPS = 250
SAVE_TOTAL_LIMIT = 3

OPTIMIZER = "adamw_torch"
LR_SCHEDULER_TYPE = "cosine"
MAX_GRAD_NORM = 1.0

USE_FP16_ON_CUDA = True
DATALOADER_NUM_WORKERS = 4

OUTPUT_DIR = "./checkpoints"
LOGGING_DIR = "./logs"
FINAL_MODEL_DIR = "./final_model"
TOKENIZER_DIR = "./tokenizer"